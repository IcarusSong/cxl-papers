# Accelerating_Performance_of_GPU-based_Workloads_Using_CXL

## 作者和出版物

### 作者
- Moiz Arif，Rochester Institute of Technology，Rochester, NY, USA，ma3890@cs.rit.edu
- Avinash Maurya，Rochester Institute of Technology，Rochester, NY, USA，am6429@cs.rit.edu
- M. Mustafa Rafique，Rochester Institute of Technology，Rochester, NY, USA，mrafique@cs.rit.edu

### 出版物
FlexScience’23, June 20, 2023, Orlando, FL, USA


## 文章背景
在高性能计算（HPC）领域，科学模拟和深度学习（DL）等工作负载通常在多GPU系统上运行，它们是内存和数据密集型的。这些工作负载依赖于主内存来补充GPU上容量有限的板载高带宽内存（HBM）。为了在较慢的设备到主机PCIe互连上实现更快的数据传输，这些工作负载通常会在主机系统上“钉住”（pin）内存。然而，在多GPU节点上，这种内存钉住（pinning）操作会限制了其他GPU可用主内存的容量，可能导致显著的数据传输开销，甚至因无法钉住所需要的内存量而导致作业失败。

Compute Express Link (CXL) 是一种新兴技术，它能以低延迟和高吞吐量的方式透明地扩展系统内存容量，并保持缓存一致性。虽然这为多GPU节点上的工作负载分配和钉住更多内存提供了可能，但使用传统的内存分配方案可能会因CXL内存上的争用而对数据吞吐量产生负面影响。

### 相关工作以及局限性
文章将相关工作分为两类：

* **分层内存的内存管理方法**：一些工作提出了针对CXL内存的透明页面放置机制，根据页面的“冷热”程度在不同内存层级间移动页面。例如，Radient提出了高效的页面放置策略，动态管理主存和NVMe之间的页面。HotBox则是一个旨在最大化本地内存命中率的内存管理子系统。
    * **局限性**：这些分层内存管理方法都没有考虑到在启用CXL的多GPU设置中分配“钉住内存”（pinned memory）的特定情况。

* **使用CXL进行内存分解**：近期的研究利用CXL来实现分解式内存系统，允许以低开销访问TB级的内存。一些研究利用基于CXL的内存作为扩展设备来提升工作负载性能。例如，CMS通过利用CXL互连来扩展内存容量，并使用近数据处理方法来最大化内部带宽。
    * **局限性**：这些研究尚未探讨通过PCIe通道连接CXL内存时可能出现的带宽瓶颈问题。

### 论文发现以及论文贡献
* **论文发现**：论文的核心发现是，尽管CXL内存为系统扩展了容量，但允许操作系统或GPU驱动程序独立于其他工作负载进行内存分配，会导致次优的内存布局。这种低效的映射会增加应用程序的执行时间，降低内存吞吐量和带宽利用率。当多个作业被调度到同一插槽（socket）的GPU上时，默认的内存分配策略（先用完DRAM再用CXL）会导致后启动的作业被迫使用带宽有限的CXL内存，从而产生严重的带宽争用问题。

* **论文贡献**：
    1. 提出了一个在Nvidia DGX-A100系统中启用CXL内存扩展的参考架构。
    2. 指出了在CXL系统上运行多个作业时，默认内存分配策略带来的性能瓶颈。
    3. 提出了一种感知调度（schedule-aware）的内存分配方法，该方法结合了多GPU系统每个插槽的内存需求，并提供高效的内存放置图以减轻内存争用。
    4. 通过多样化的作业配置文件和系统配置对所提方法进行了评估，模拟结果显示，与现有方法相比，其数据传输时间最多可降低65%。

### 方法策略
* **如何利用这个发现**：研究团队利用其关于“CXL内存争用”的发现，设计了一种能够智能地跨越主内存（DRAM）和CXL内存层级进行分配的策略，目标是最大化数据传输速率并减少总执行时间。

* **遇到了什么困难**：主要困难是CXL内存上的带宽争用。当多个GPU同时从连接在同一个PCIe链路上的CXL设备读写数据时，CXL内存的带宽会被所有GPU平分，从而在PCIe互连上产生争用，这会严重影响那些被分配到CXL内存的作业的读写吞吐量。

* **设计了什么方法-策略**：论文提出了一种名为“感知调度的数据分配方法” (Schedule Aware Data Allocation Approach)，其过程记录在算法1中。
    1. **输入**：算法的输入包括节点上的插槽数、待执行的作业列表（含作业ID、总内存需求、所需GPU数）、可用的GPU列表、每个插槽可用的DRAM和CXL内存量，以及DRAM、PCIe和CXL的带宽。
    2. **作业选择**：首先，从作业队列中选择一批可以在当前可用GPU资源上执行的作业。
    3. **计算内存溢出 (Spill)**：对于每个被调度的作业，算法会计算其在公平共享DRAM的情况下，超出DRAM容量的内存需求部分，这部分被称为“spill”。
    4. **计算CXL分配量**：基于计算出的“spill”量、CXL可用内存以及DRAM、PCIe和CXL的各自带宽，算法会计算出可以被高效分配到CXL设备上的内存量，同时确保同一插槽上的其他作业不会面临DRAM饥饿问题。
    5. **生成分配计划**：最后，算法为每个作业确定在DRAM和CXL上的具体内存分配量，并更新可用的内存资源，最终输出一个高效的多层内存分配计划。

### 实验设置与实验结果
* **实验平台**：实验并非在真实的CXL硬件上进行，而是通过一个用Python编写的模拟模型来完成的。该模拟器在一个配备两颗Intel Xeon Gold 6240R处理器和192 GB主内存的服务器上运行。模拟的系统架构扩展自Nvidia DGX-A100，每个插槽（socket）有4个GPU，并通过PCIe Gen 4.0连接到主机。

* **效果如何**：该方法效果显著。与默认的“朴素”（Naive）分配方法相比，新方法将数据传输开销降低了15.4%至61.2%。在变化的PCIe带宽测试中，新方法比“朴素”和“统一”（Uniform）分配方法平均好65.35%和21.3%。总体而言，模拟结果表明，该方法相比现有分配方式，数据传输开销最多可降低65%。

* **对比的指标**：主要的性能评估指标是数据传输时间 (Data Transfer Time)。

* **对比是否公平？是否全面？**
    * **公平性**：对比是公平的。论文将自己的方法与两种基准方法进行了比较：
        1. **Naive (朴素)**：默认的内存分配方式，即先用尽主内存，再使用CXL内存。
        2. **Uniform (统一)**：调度器将可用的主内存在所有GPU之间均匀分配。
        这两种方法代表了当前系统可能采用的分配策略，为评估新方法的优越性提供了合理的基线。
    * **全面性**：实验设计较为全面。研究人员通过改变三个关键变量来评估不同方法的性能：
        1. 每个插槽可用的主内存容量。
        2. 可用的PCIe带宽（模拟从PCIe 3.0开始的不同PCIe代际）。
        3. 不同程度的CXL惩罚（CXL penalty），即CXL协议只能实现PCIe实际带宽的60%-90%。

* **实验设计分析**：
    * **解决问题效果**：图2 (a, b, c)清晰地显示，在所有测试场景下，“Our Approach”的数据传输时间均显著低于“Naive”和“Uniform”方法，证明了其有效性。
    * **比相关工作好**：实验结果一致表明，该方法优于作为对比的“Naive”和“Uniform”基准方法。
    * **效果合理性**：其效果之所以更好是合理的，因为它通过智能感知作业调度和系统资源状况，主动避免了“Naive”方法中的资源抢占和“Uniform”方法中可能出现的低效分配，从而最小化了由带宽争用导致的数据传输瓶颈。
    * **消融实验**：“Naive”和“Uniform”方法的对比可以看作是一种消融分析。它证明了仅仅增加CXL内存（如“Naive”方法）或采用简单的公平分配策略（如“Uniform”方法）是不够的，必须采用论文中提出的更智能的、感知调度的分配策略才能取得最佳效果。

### 前提假设与局限性
* **前提假设**：论文的设计和评估基于以下几个明确的假设：
    1. 作业队列中总是有足够的作业来占用系统上所有可用的GPU。
    2. 每个作业所需的内存都在应用程序初始化时被钉住（pinned），以实现更高的DMA传输速率，并且在作业完成前不能调整大小。
    3. 所有作业都指定了唯一的内存需求，并且在整个执行过程中内存占用保持不变。
    4. 所有作业都是内存密集型的，会持续地对主内存和CXL内存层中的数据进行读写。
    5. CXL设备具有足够大的容量来满足所有活跃和已调度作业的内存需求。

* **局限性**：
    1. **模拟而非实物**：评估是基于初步的模拟，而不是在真实的CXL硬件上进行的，这可能无法完全捕捉到真实世界的所有复杂性。
    2. **带宽瓶颈**：论文承认，连接CXL设备的有限PCIe带宽本身就是一个瓶颈，尤其是在默认调度下。
    3. **静态内存分配**：当前的方法不支持动态内存大小调整，这是未来计划改进的方向。