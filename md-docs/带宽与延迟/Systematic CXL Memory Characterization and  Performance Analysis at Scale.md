
# Systematic CXL Memory Characterization and  Performance Analysis at Scale
## 0. 作者以及出版物
### 作者
**主要研究团队 (弗吉尼亚理工大学):**
*   Jinshu Liu (刘锦澍)
*   Hamid Hadian
*   Yuyue Wang
*   Xun Jian
*   Sam H. Noh
*   Huaicheng Li (李怀诚) - 通讯作者

**行业合作者:**
*   **Daniel S. Berger** (微软 & 华盛顿大学)
*   **Marie Nguyen** (三星)
*   
### 出版物
ASPLOS'25
## 1. 文章背景

随着内存密集型应用（如大数据、AI、内存数据库）的快速发展，对内存容量的需求急剧上升。然而，传统的DRAM技术面临扩展瓶颈（DRAM scaling challenges），成本和功耗问题日益突出。

**Compute Express Link (CXL)** 作为一种新兴的互联协议，为解决这一问题提供了关键方案。它允许CPU通过高速的PCIe物理层连接到外部的内存扩展设备，实现服务器级别乃至机架级别的内存池化和扩展。
![](https://pic1.imgdb.cn/item/6880870658cb8da5c8ce3425.png)
尽管CXL潜力巨大，但它也引入了比主板直连DRAM（socket-local DRAM）更高的访问延迟（通常在200-400ns，甚至更高），并且不同厂商、不同拓扑结构（如直连、经过NUMA、经过Switch）的CXL设备表现出显著的**异构性（heterogeneity）**。目前，学术界和工业界对于CXL在真实系统和大规模应用下的性能影响，特别是其详细的性能特征（如延迟稳定性）及其对CPU和应用效率的深层影响，仍然缺乏系统和全面的理解。

本文的目标就是填补这一空白，通过大规模的实验和创新的分析方法，系统地、深入地刻画CXL内存的性能，并分析其对各种工作负载的影响。

### 2. 相关工作以及局限性

文章首先肯定了之前的一些研究工作，但明确指出了它们的局限性：

*   **研究范围有限 (Limited Scope):** 以前的研究通常只测试少数几个云/HPC工作负载，并且使用的CXL设备种类和延迟配置也较少，这使得它们的结论普适性不强。
*   **分析粒度粗糙 (Coarse-grained Analysis):** 大部分工作主要关注平均延迟（average latency）和带宽（bandwidth）这两个宏观指标，而忽略了其他关键的性能维度。
*   **忽视了关键问题 (Overlooked Critical Aspects):**
    1.  **性能稳定性（尾延迟, Tail Latency）:** 几乎没有工作深入研究CXL的尾延迟。尾延迟对于云服务等对服务质量（QoS）敏感的应用至关重要，但之前被普遍忽视。
    2.  **CPU对CXL延迟的容忍度:** 缺乏大规模的评估来揭示不同类型的工作负载和CPU架构对CXL带来的额外延迟的真实容忍度。
    3.  **缺乏系统的性能诊断方法:** 当应用在CXL上变慢时，现有方法难以精确地定位和量化性能瓶颈的根本原因。例如，Intel的TMA（Top-down Microarchitecture Analysis）方法虽然强大，但它无法进行**差分分析**（即精确对比CXL与本地DRAM的性能差异来源），也无法将底层硬件事件与上层应用性能放缓精确关联。

### 3. 论文发现以及论文贡献

本文的核心贡献是提出了一个名为**MELODY**的框架，并基于此进行了一系列深入研究。

#### **核心发现 (Key Findings):**

1.  **CXL设备性能并非“生而平等”**: 首次揭示并量化了不同CXL设备间不仅在平均延迟和带宽上有差异，更在**尾延迟**上有巨大差异。某些CXL设备即使在低负载下也会出现微秒（µs）级别的尾延迟。
2.  **工作负载对CXL的敏感性**:
   ![](https://pic1.imgdb.cn/item/6880873b58cb8da5c8ce3573.png)
   ![](https://pic1.imgdb.cn/item/6880876e58cb8da5c8ce372a.png)
    *   许多工作负载（约占1/3）可以很好地容忍CXL的延迟（<10%的性能下降），使其成为本地DRAM的可行替代方案。
    *   但工作负载性能会随着CXL延迟的增加呈**超线性（super-linearly）恶化**。
  ![](https://pic1.imgdb.cn/item/688087b258cb8da5c8ce3928.png)
    *   设备的高尾延迟会显著影响应用性能，尤其是在跨NUMA节点访问CXL时（CXL+NUMA），可能导致出乎意料的性能急剧下降。
![](https://pic1.imgdb.cn/item/6880878b58cb8da5c8ce382c.png)
1.  **CPU预取器在CXL下效率低下**: 首次全面分析并证实，CPU硬件预取器（CPU Prefetcher）因CXL的超长延迟而变得“不及时”，导致预取效果大打折扣，这是造成CXL下缓存性能下降（cache slowdown）的主要原因。
2.  **性能下降的根本原因可量化**: 发现工作负载在CXL上的性能下降，主要源于CPU后端（CPU backend）中因等待内存访问而产生的**额外停顿周期（stall cycles）**。

#### **主要贡献 (Contributions):**

1.  **MELODY框架**: 提出了一个大规模、系统的CXL性能表征和分析框架，其评估范围史无前例（265个工作负载，4个真实CXL设备，7种延迟配置，5个CPU平台）。
2.  **首次深入研究CXL尾延迟**: 首次披露、量化并分析了CXL的尾延迟，并评估了其对CPU效率和真实应用性能的影响。
![](https://pic1.imgdb.cn/item/6880884958cb8da5c8ce3f41.png)
3.  **创新的根因分析方法(SPA)**: 提出了**SPA (Stall-based CXL Performance Analysis)**，一种新颖的、轻量级（仅用9个CPU性能计数器）且高精度（>95%的工作负载误差<5%）的性能诊断方法。SPA能够精确地将CXL引起的性能下降分解到CPU流水线的不同组件上（如DRAM访问、存储、L1/L2/LLC缓存等）。
4.  **开源工件**: 开源了MELODY框架的工具和数据集，以促进社区的后续研究。

### 4. 方法策略（如何利用这个发现，遇到了什么困难？设计了什么方法-策略）

本文的核心方法论是**SPA (Stall-based CXL Performance Analysis)**。

*   **利用的发现**: 本文发现，工作负载在CXL上运行比在本地DRAM上慢，其根本原因是CPU为了等待更慢的内存访问而产生了更多的停顿。总运行周期的增加量（Δc）约等于总停顿周期的增加量（Δs）。因此，性能放缓比例 `S = Δc / c ≈ Δs / c`。

*   **遇到的困难**:
    1.  如何精确测量和分解这些“额外”的停顿周期（Δs）？
    2.  如何将这些底层的停顿与具体的硬件组件（如L1缓存、L2缓存、DRAM）关联起来？
    3.  现有工具（如TMA）无法进行这种精确的差分量化分析。

*   **设计的策略 (SPA)**:
    1.  **差分分析 (Differential Analysis)**: SPA的核心思想是**不**分析CXL或DRAM设置下的绝对性能，而是分析两者之间的**差异（Delta, Δ）**。通过测量运行在CXL和DRAM上时各项性能计数器的差值，来消除背景噪声，精确捕捉由CXL引入的影响。
    2.  **构建停顿模型 (Stall Breakdown Model)**: SPA精心挑选了9个Intel CPU性能计数器（如`BOUND_ON_LOADS`, `STALLS_L1D_MISS`等），这些计数器能够分别度量因存储缓冲满、L1/L2/L3缓存未命中、等待DRAM/CXL数据返回等原因造成的停顿。通过这些计数器的差值，可以将总的额外停顿（Δs）分解为来自不同组件的贡献，如 `Δs ≈ Δs_store + Δs_L1 + Δs_L2 + Δs_L3 + Δs_DRAM`。
    3.  **量化性能下降来源**: 将每个组件的停顿增加量除以基线总周期，就得到了该组件对总体性能下降的贡献度。例如，`S_DRAM = Δs_DRAM / c` 就是由DRAM/CXL访问变慢直接导致的性能下降部分。
    4.  **时序分析 (Period-based Analysis)**: 为了分析动态变化的工作负载，SPA还被扩展为基于时间周期的分析。通过将基于时间的采样数据映射到基于指令计数的周期，可以动态地展示在程序执行的不同阶段，性能下降的来源是如何变化的。

### 5. 实验设置与实验结果

*   **实验平台**:
  ![](https://pic1.imgdb.cn/item/688088ff58cb8da5c8ce4574.png)
    *   **CPU**: 使用了5种不同的服务器平台，涵盖Intel的SPR (Sapphire Rapids), EMR (Emerald Rapids), SKX (Skylake) 三代处理器。
    *   **真实CXL设备**: 测试了来自不同供应商的**4款真实CXL设备**（论文中称为CXL-A, B, C, D）。其中3款是**ASIC**，1款是**FPGA**，这覆盖了主流的实现方式。
    *   **NUMA模拟**: 为了扩展延迟范围，研究人员还利用多路服务器的NUMA架构来**模拟**不同延迟的CXL环境（如140ns, 190ns, 410ns）。这是一种被广泛接受的模拟远端内存访问的方法。

*   **效果与对比**:
    *   **对比指标**:
        *   **设备级**: 平均延迟、加载延迟、尾延迟分布（CDF）、带宽-延迟曲线、不同读写比下的带宽。
        *   **应用级**: **性能放缓比例 (Slowdown)**，即 `(T_cxl / T_dram - 1) * 100%`。
        *   **SPA分析**: 性能放缓的分解图（breakdown chart），显示各组件的贡献。
    *   **对比公平性**:
        *   基线是**Socket-local DRAM**，这是最公平、最标准的对比基准。
        *   在需要时，会进行特殊配置以确保公平。例如，在对比CXL和NUMA的带宽特性时，他们通过减少服务器的DIMM数量来匹配CXL设备的内存通道数，进行了公平的比较。
    *   **全面性**: 实验覆盖面非常广，包括265个来自不同领域（云、图计算、数据分析、ML/AI、SPEC CPU等）的工作负载，确保了结论的普适性。
    *   **实验结果**:
       ![](https://pic1.imgdb.cn/item/688088aa58cb8da5c8ce41eb.png)
        *   **效果好**: SPA方法被验证为高度准确（图11），与真实的应用性能放缓差异极小。它成功地用于性能调优，例如，通过分析`605.mcf`，定位到两个关键数据对象，并将它们移至本地DRAM后，性能放缓从13%降至2%。
        *   **比相关工作好**: SPA相比传统方法，提供了更精确、更可解释的根因分析，解决了TMA等工具无法进行差分量化分析的痛点。
        *   **消融实验**: 一个关键的消融实验是**禁用硬件预取器**。实验发现，禁用后，由缓存（L1/L2/L3）引起的性能放缓几乎消失了。这强有力地证明了**CXL下的缓存性能下降确实是由预取器失效引起的**，验证了SPA分析的合理性。

### 6. 前提假设与局限性

*   **前提假设**:
    1.  **后端停顿主导**: SPA假设性能下降主要由CPU后端停顿（backend stalls）引起，前端停顿（frontend stalls）的影响可以忽略。论文通过实验数据（图11b）验证了这个假设在绝大多数情况下是成立的。
    2.  **计数器可用性**: SPA依赖于特定的Intel CPU性能计数器。将其移植到其他架构（如AMD, ARM）需要找到功能等价的计数器。
    3.  **时序分析平滑性**: 基于周期的时序分析假设在短采样间隔（1ms）内计数器是平滑增长的，这对于细粒度分析是一个合理的近似。

*   **局限性**:
*   1. **单一介质**：文章实验对象仅对比单一的CXL内存与DDR内存，未考虑分层和交织情况。
    1.  **CXL控制器是黑盒**: 由于商业CXL内存控制器（MC）是黑盒，研究人员无法直接观测其内部状态（如调度队列、刷新操作等）。他们只能通过外部表现（如尾延迟）来推断其内部行为。论文提到，未来CXL 3.0中的CPMU（Performance Monitoring Unit）有望解决此问题。
    2.  **热节流测试不充分**: 出于保护设备的目的，研究人员没有进行极限的温度压力测试，因此无法完全排除热节流（thermal throttling）是导致尾延迟的一个潜在原因，尤其对未来功耗更高的设备。
    3.  **研究范围**: 研究主要集中在CXL 1.1协议（因CPU支持限制）和Type-3内存扩展设备上。对于CXL 2.0/3.0的新特性以及其他类型的CXL设备（如Type-1/2）未做深入探讨。