
# Hello bytes, bye blocks: PCIe storage meets compute express link for memory expansion

## 作者以及出版物
### 作者
Myoungsoo Jung， **KAIST**

### 出版物
HotStorage ’22

## **1. CXL技术的兴起与块存储的缺席**
**背景**  
Compute Express Link（CXL）作为一种开放、缓存一致的互连协议，自诞生以来被视为异构计算系统的革命性技术。它通过整合CPU、加速器和内存设备，构建统一的缓存一致性内存空间，尤其擅长内存池化（Memory Pooling）。然而，CXL最初的设计主要面向DRAM和持久内存（PMEM），**未将块存储设备（如SSD）纳入其核心支持范围**。这一局限性引发关键问题：  
- **块存储能否通过CXL实现内存级性能？**  
- **如何将块语义（Block Semantics）转换为字节语义（Byte Semantics）？**  


## **2. 核心问题：块存储为何需要CXL？**
论文提出四个关键问题，直指传统PCIe存储的痛点：  
### **(1) 块语义与内存语义的冲突**  
- **块存储的固有缺陷**：传统SSD通过NVMe协议以块（通常4KB）为单位读写数据，与CPU的字节寻址模式不兼容。  
- **内存扩展需求**：新兴应用（如大规模图计算、AI训练）需TB级内存，而DRAM成本高昂，亟需低成本扩展方案。  

### **(2) PCIe的非缓存访问限制**  
- **性能瓶颈**：PCIe存储通过内存映射I/O（MMIO）暴露地址空间，但CPU无法缓存此类请求，导致每次访问需直接穿透PCIe总线，延迟高且带宽利用率低。  
- **示例**：PCIe Gen5 x16的理论带宽为64GB/s，但因非缓存访问，实际有效带宽可能不足50%。  

### **(3) 缓存一致性的缺失**  
- **多设备协同难题**：传统PCIe设备无法参与CPU缓存一致性协议，导致多主机或加速器共享存储时需复杂同步机制。  

### **(4) 存储资源池化的潜力**  
- **目标**：通过CXL将分布式SSD聚合成全局内存池，支持细粒度分配与低延迟访问。  


## **3. PCIe存储的困境：为何无法直接作为内存扩展器？**
尽管PCIe存储具备大容量和低成本优势，其作为内存扩展器存在根本性缺陷：  
### **(1) 非缓存访问的性能代价**  
- **强制同步访问**：CPU对PCIe BAR（Base Address Register）的访问需绕过缓存，导致频繁的PCIe事务和更高的延迟。  
- **对比实验**：在相同工作负载下，PCIe存储的访问延迟是本地DRAM的10倍以上（见论文§5）。  

### **(2) 块语义的兼容性挑战**  
- **粒度不匹配**：内存操作以缓存行（通常64B）为单位，而SSD的最小读写单元为4KB，造成读写放大。  
- **地址转换开销**：需额外FTL（Flash Translation Layer）将字节地址映射为物理块，增加延迟。  

### **(3) 缺乏缓存一致性机制**  
- **多设备竞争**：若多个CPU核或加速器并发访问同一PCIe存储区域，需软件维护一致性，复杂度高且易出错。  


## **4. CXL的革新：将PCIe存储集成到内存层次**
### **(1) 设备类型选择：为何是Type 3？**  
CXL定义三种设备类型（Type 1~3），论文主张将PCIe存储作为**Type 3设备**，原因包括：  
- **协议简化**：仅需支持CXL.mem（内存读写）和CXL.io（设备管理），无需复杂的CXL.cache（缓存一致性）。  
- **扩展性优势**：Type 3支持多设备级联，而Type 2因缓存一致性限制仅支持单设备连接。  
- **硬件改造成本低**：复用现有NVMe控制器逻辑，仅需新增CXL协议解析模块。  

### **(2) 系统集成方案**  
![](https://pic1.imgdb.cn/item/67fd018488c538a9b5d0e068.png)
- **内存映射**：通过CXL Root Port（RP）将SSD的HDM（Host-Managed Device Memory）映射到主机物理地址空间，实现字节寻址。  
    - 内存映射过程：
        - 步骤1：主机启动时，枚举连接到CXL RP的CXL设备，并初始化这些设备。主机将设备的内存空间（HDM和BAR）映射到系统内存中。
        - 步骤2：CXL RP在系统内存中预留了一部分空间用于CXL设备的内存映射。HDM被映射到可缓存的内存地址空间，这样用户可以通过加载/存储指令访问它。BAR也被映射到系统内存中，用于设备的配置和控制。
        - 步骤3：当应用程序访问映射到HDM的内存时，CXL RP会生成CXL flit（一种CXL消息），并通过CXL.mem发送到目标CXL存储控制器。CXL控制器解析flit并提取请求信息，然后与设备固件协作来处理请求。设备固件管理设备的内部操作，如数据读写和缓存管理。

- **访问流程**：  
  1. 主机通过load/store指令访问CXL内存区域。  
  2. CXL RP生成CXL Flit（协议数据单元），经PCIe物理层传输至存储设备。  
  3. SSD内部CXL控制器解析请求，从DRAM缓存或后端介质返回数据。  

### **(3) 存储架构改造**  
- **DRAM缓存优化**：SSD内部DRAM作为写回缓存，结合预取算法隐藏后端介质延迟。  
- **FTL协同设计**：将FTL地址转换逻辑卸载至CXL控制器，减少主机CPU开销。  


## **5. 性能验证：CXL存储的潜力与局限**
### **(1) 测试环境**  
- **硬件原型**：基于FPGA模拟CXL主机与OpenExpress SSD（模拟Z-NAND介质）。  
- **对比方案**：本地DRAM、传统PCIe存储、CXL扩展存储。  

### **(2) 关键结果**  
![](https://pic1.imgdb.cn/item/67fd01bd88c538a9b5d0e086.png)
- **高局部性场景（α=0.001）**：CXL存储延迟接近DRAM（1.73 vs. 1.0周期），因缓存命中率高。  
- **随机访问场景（α=1）**：CXL延迟达344.9周期，显著高于DRAM（4.1周期），反映后端介质延迟的影响。  
- **结论**：CXL存储适用于局部性较强的负载，但需优化尾部延迟。  


## **6. 存储分离：构建可扩展的CXL网络**
![](https://pic1.imgdb.cn/item/67fd01d288c538a9b5d0e088.png)
### **(1) 拓扑设计**  
- **单交换机架构**：通过CXL Switch连接多个SSD，支持主机动态映射存储资源。  
- **多级交换机网络**：扩展至树状结构，支持数百个SSD设备池化。  

### **(2) 虚拟化与资源共享**  
- **MLD（Multiple Logical Device）**：将单个物理SSD虚拟化为多个逻辑设备，分别映射到不同主机的内存空间。  
- **优势**：支持多租户隔离、资源按需分配，避免存储带宽争用。  


## **7. 延迟优化：确定性（Determinism）与缓冲能力（Bufferability）**
### 延迟波动（Latency Fluctuation）
CXL Type 3设备设计初衷是内存池化，而非块存储。当PCIe存储作为CXL内存扩展时，其内部任务（如垃圾回收、FTL映射更新）可能导致响应时间不可预测，影响主机性能。

### 数据持久性（Data Persistence）
现有CXL通过全局持久刷新（GPF）机制强制将数据写回存储介质，但GPF会导致大规模DRAM缓冲数据同时刷写，引发显著延迟。


为了解决上面两个延迟问题，本文提出了两个特性：确定性和缓冲性。这些特性可以标注在CXL消息上，并向底层CXL控制器提示主机语义。CXL允许Type 3管理CXL.io以满足各种特定的I/O需求，CXL.mem也有一个保留字段，可用于标注。

### **(1) 确定性（DT/ND）**  
- **DT（Deterministic）**：要求存储设备立即处理请求，绕过垃圾回收等后台任务。  
  - **用例**：事务提交、锁同步等低延迟敏感操作。  
- **ND（Non-Deterministic）**：允许请求异步处理，存储设备在空闲时段执行内部任务。  
  - **用例**：日志写入、数据预取等后台任务。  

### **(2) 缓冲能力（BF/NB）**  
- **BF（Bufferable）**：数据暂存于SSD DRAM，延迟写回介质。  
  - **用例**：临时数据、高局部性负载。  
- **NB（Non-Bufferable）**：强制数据持久化到存储介质。  
  - **用例**：关键元数据、事务提交。  

### **(3) 组合策略示例**  
- **数据库事务**：  
  - 日志写入：BF+ND（缓冲日志，异步持久化）。  
  - 事务提交：NB+DT（立即持久化，绕过内部任务）。  
- **科学计算**：  
  - 中间结果：BF+ND（利用DRAM加速迭代计算）。  
  - 检查点保存：NB+ND（异步但确保持久化）。  


## **8. 总结与展望**
### **(1) 核心贡献**  
- **理论突破**：证明CXL可通过Type 3设备将块存储转换为字节可寻址内存。  
- **实践验证**：通过原型系统验证CXL存储的可行性，揭示性能边界。  
- **协议扩展**：提出Determinism与Bufferability，为CXL存储优化提供新范式。  

### **(2) 未来方向**  
- **硬件自动化**：在SSD控制器中集成CXL协议硬核，减少FPGA原型开销。  
- **软件生态**：开发CXL存储管理工具链（如内存分配器、持久化库）。  
- **网络优化**：研究CXL交换机拥塞控制算法，提升多设备并发访问效率。  

### **(3) 产业意义**  
CXL存储扩展技术有望推动“内存-存储”层级融合，为AI、大数据等场景提供低成本、高容量的内存解决方案，同时为下一代存储架构（如存算一体）奠定基础。
