
# Direct Access, High-Performance Memory Disaggregation with DIRECTCXL

## 作者以及出版物
### 作者
Donghyun Gouk, Sangwon Lee, Miryeong Kwon, Myoungsoo Jung，**KAIST**

### 出版物
ACT24

---

## 1. 内存解耦背景

随着数据中心和大规模计算任务对内存容量及利用率要求的不断提高，**内存解耦（Memory Disaggregation）** 作为一种新型架构，被提出以实现内存资源池化、弹性扩展和高效共享。通过将内存资源从计算节点中分离出来，可以在多个主机之间灵活调度内存，从而提高整体系统的资源利用率。

---

## 2. 现有方法：基于页面和基于对象的内存解耦

### 2.1 基于页面的方法  
- **原理：**  
  该方法依赖虚拟内存技术，当主机发生 page fault 时，将本地缓存的页数据交换到远程内存节点。
- **优点与问题：**  
  具有透明性，不需应用修改，但需要频繁处理页故障、上下文切换以及数据复制开销，容易引起 I/O 放大效应。

### 2.2 基于对象的方法  
- **原理：**  
  以 key-value store 的形式直接管理远程内存，应用程序需要修改源代码，以适配新的数据访问接口。
- **优点与问题：**  
  可以规避页级调度的开销，但由于核心设计依赖数据的封装和额外运算（如哈希处理），造成额外的计算开销和延迟，且需要较大改动应用程序代码。

### 2.3 共性问题：对 RDMA 的依赖  
- 不论是基于页面还是基于对象的内存解耦方法，目前都采用了 **RDMA（Remote Direct Memory Access）** 技术来传输数据。  
- **问题在于：**  
  RDMA 在实际操作时需要经过两次 DMA 操作、协议转换以及软件栈的介入，导致数据复制和网络管理开销较大，从而使得远程内存访问延迟比本地 DRAM 延迟高出多个数量级。

---

## 3. RDMA 的局限
![](https://pic1.imgdb.cn/item/67fcf92988c538a9b5d0dbec.png)

当前大多数内存解耦方案都依赖于 RDMA 技术，但其在实际应用中存在明显的延迟和数据传输开销问题。图中详细展示了 RDMA 数据传输的关键步骤，揭示了其主要局限性，具体包括：

1. **内存区域注册与数据拷贝开销**  
   - **注册阶段：**  
     主机和远程内存节点在开始传输前，需要将各自要使用的数据区域注册到 RDMA 网卡（RNIC），并在网卡内部建立一个内存转换表（MTT）。这一步确保了虚拟地址与物理地址的映射，但同时也增加了配置和管理开销。  
   - **数据拷贝：**  
     为确保数据直接由 RNIC 进行 DMA 传输，必须先将应用数据复制到注册的内存缓冲区（pinned memory），传输完成后在接收端还需要再次拷贝数据到应用程序的缓冲区。这一系列拷贝操作不仅增加了整体延迟，而且在高负载和小数据传输时尤为明显。

2. **协议转换和网络传输延迟**  
   - RDMA 通过网络直接进行数据传输（例如 InfiniBand），图中显示当主机发起一个含有目标地址、数据长度等参数的请求时，RNIC将利用内部 DMA 机制将数据直接写入远程内存。  
   - 尽管 RDMA 能够绕过传统 TCP/IP 协议栈，但仍需经过两次 DMA 操作，并处理相关协议转换，这使得单次访问的硬件延迟（例如图 6 中的 2700+ cycles）明显高于直接访问本地 DRAM。

3. **整体延迟分布和尾延迟问题**  
   - 如图所示，RDMA 的完整路径不仅包含上面提到的硬件延迟，还叠加了软件栈（如 libibverbs）的额外开销。  
   - 实际测量发现，在应用级别统计中 RDMA 的延迟可能达到 4000+ cycles，而在内存层级视角下，仅硬件传输部分的延迟大约 2000 cycles。这种多级叠加带来的延迟波动（tail latency）使得 RDMA 在延迟敏感型应用中的表现较差。

- **总结：**  
  RDMA 的整体开销使得基于其的内存解耦解决方案在延迟上远不能与直接访问本地 DRAM 相比。

---

## 4. 本文提出的 Direct CXL 方案

为了解决上述 RDMA 高延迟以及复杂软件栈带来的问题，文章提出了 **Direct CXL**，其核心思想是利用 CXL（Compute Express Link）标准中的内存协议（CXL.mem），实现主机与远程内存之间的直接、低延迟通信。其设计主要涵盖如下几个方面：

### 4.1 连接方式  
![](https://pic1.imgdb.cn/item/67fcfa9788c538a9b5d0dce4.png)
- **硬件设计：**  
  - 直接将远程内存节点作为一个纯被动模块，通过 PCIe 总线与主机的 CXL root port 相连。  
  - 在远程设备侧使用多个 DRAM 控制器管理 DRAM 模块，CXL 控制器负责将 PCIe 传输的数据转换为 DRAM 请求。  
  - 通过 CXL switch 建立虚拟层级，实现主机与多个远程内存节点的灵活组合。
  ![](https://pic1.imgdb.cn/item/67fcfaad88c538a9b5d0dcf1.png)
  
### 4.2 软件运行时  
![](https://pic1.imgdb.cn/item/67fcfac788c538a9b5d0dd0b.png)
- **设计思路：**  
  - 提供一个简洁的 CXL 软件运行时，将远程内存映射为 **cxl-namespace**，类似于内存映射文件（mmap）。  
  - 该运行时通过 ioctl 接口管理 namespace 的分配和映射，无需应用级别修改代码，直接支持 load/store 指令访问远程内存。
  
### 4.3 原型实现  
![](https://pic1.imgdb.cn/item/67fcfaf388c538a9b5d0dd23.png)
- **原型构建：**  
  - 系统基于 FPGA 实现的 CXL 内存 add-in 卡和基于 RISC-V 的主机处理器搭建；  
  - 利用 PCIe backplane 和自研 CXL switch，将多台主机与多块 CXL 设备连接成一个内存池系统；  
  - 软件上通过 Linux 5.13 和自研驱动实现 CXL 枚举、内存映射及数据传输功能。

---

## 5. Direct CXL 与 RDMA 的对比（第四部分评估）

**评估部分**详细通过微基准测试和真实工作负载测试对 Direct CXL 和基于 RDMA 的内存解耦方案进行了对比分析，主要对比内容如下：

- **延迟方面：**
  - **硬件路径延迟：**  
  ![](https://pic1.imgdb.cn/item/67fcfb2b88c538a9b5d0dd48.png)
    Direct CXL 在单次 64B 数据读取测试中，延迟仅为 328 cycles，而 RDMA 在相同测试条件下延迟约 2705 cycles，从硬件实现来看，Direct CXL 快了大约 8.3 倍。
  - **实际应用延迟：**  
  ![](https://pic1.imgdb.cn/item/67fcfb5f88c538a9b5d0dd5d.png)
    当考虑软件栈和数据复制开销时，RDMA 的延迟明显上升，达到 4000+ cycles，而 Direct CXL 则因省去了大量软件干预和数据复制，延迟更稳定、接近本地 DRAM 的访问特性。
  - **内存层级分析（图 8）：**  
  ![](https://pic1.imgdb.cn/item/67fcfb9288c538a9b5d0dd72.png)
    以内存层级视角看，RDMA 的延迟在 2000+ cycles，而 Direct CXL 则为 328 cycles，显示出其在延迟分布和 tail latency 方面的优势。

- **可扩展性及并发性：**
  - **RDMA：** 软件库、数据复制和网络处理瓶颈明显，系统在高并发访问下性能急剧下降。
  - **Direct CXL：** 能够利用直接 load/store 指令和高效的内存映射机制，在多核心并发访问下保持较低延迟和线性增长的带宽，极大地提高了整体可扩展性。

- **真实工作负载表现：**
![](https://pic1.imgdb.cn/item/67fcfbd488c538a9b5d0dd9c.png)
  - 在 DLRM、MemDB 和图计算等多种真实工作负载测试中，Direct CXL 相对于基于页交换的 Swap 模型和基于对象的 KVS 模型，分别提升了 3 倍和 2.2 倍的性能，说明其在实际应用场景中具有更高的优势和适应性。

---

## 总结

文章首先讨论了内存解耦的重要背景及当前常用的两种基于页面与基于对象的方法，这两类方法均依赖于 RDMA 技术，而 RDMA 本身因涉及多重数据复制和复杂协议转换而导致较高延迟。接着，文章提出 Direct CXL 方案，通过直接将远程内存映射到主机系统中，并提供轻量级的软件运行时，实现了直接访问远程内存的目标。最后，通过微基准测试和实际工作负载测试，文章详细证明了 Direct CXL 在延迟、带宽及系统可扩展性等方面显著优于基于 RDMA 的内存解耦方案。