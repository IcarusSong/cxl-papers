
# Logical Memory Pools:  Flexible and Local Disaggregated Memory
## 作者及出版物
### 作者
- **Emmanuel Amaro**: VMware Research 
- **Stephanie Wang**: 加州大学伯克利分校 (UC Berkeley) 
- **Aurojit Panda**: 纽约大学 (NYU) 
- **Marcos K. Aguilera**: VMware Research
### 出版物
HotNets ’23, November 28–29, 2023, Cambridge, MA, USA

## 文章背景

该研究的背景是数据中心对内存资源日益增长的需求以及对更高资源利用率和更低拥有成本的追求。传统的服务器架构将计算和内存紧密耦合，导致内存资源无法在不同服务器间共享，造成了资源浪费。为了解决这个问题，“内存分离”（Memory Disaggregation）技术应运而生，它旨在将内存汇集成一个可由多个服务器访问的共享池。早期的内存分离方案多基于软件实现（例如，使用RDMA），但这些方案因为需要软件显式发出IO请求，延迟较高且CPU开销大。随着新兴的CXL（Compute Express Link）技术获得广泛的行业支持，基于硬件的内存分离成为可能。CXL允许处理器通过PCIe总线直接以“加载-存储”（load-store）的方式访问外部内存，这种方式延迟更低、开销更小。当前基于CXL的内存分离方案主要集中在物理内存池（Physical Memory Pools）上，即创建一个与服务器物理上分离的、独立的内存设备池。这篇论文正是在此背景下，对物理内存池方案提出挑战，并提出了一种新的架构。

## 相关工作以及局限性

文章主要针对两种现有的内存分离方法，并指出了它们的局限性：

* **软件内存分离 (Software Memory Disaggregation)**: 这类工作通过RDMA等技术实现远程内存访问。
    * **局限性**: 访问内存需要软件（应用库或操作系统）发起IO请求，并通过网络堆栈处理完成，整个过程缓慢且与CPU的流水线、乱序执行、预取等架构特性不匹配，导致CPU开销大，性能不佳。

* **基于CXL的物理内存池 (Physical Memory Pools)**: 这是当前CXL内存分离的主流提议，它使用一个物理上独立的盒子作为共享内存池，与所有服务器分离。
    * **局限性**:
        1. **额外成本**: 需要为内存池购置额外的硬件（如电源、主板、CPU或ASIC/FPGA）、占用额外的机架空间，并消耗交换机端口。
        2. **性能损失**: 即使使用CXL，访问物理池的内存也比访问本地内存慢3-10倍。
        3. **缺乏近内存计算能力**: 物理内存池本身没有CPU或加速器，虽然可以添加，但这会进一步加剧成本问题。
        4. **缺乏灵活性**: 本地内存与池化内存的比例在部署后是固定的。如果工作负载的工作集大小超过了本地内存，性能会急剧下降，这迫使部署时可能需要过度配置本地内存，从而违背了内存分离的初衷。

## 论文发现以及论文贡献

**核心发现/提议**:
论文的核心提议是构建一种名为逻辑内存池（Logical Memory Pools, LMPs）的新型内存分离架构。其关键思想是，不使用一个独立的物理内存池，而是通过划分每台服务器的部分本地内存来共同组成一个共享内存池。这样，每台服务器的内存被逻辑上划分为“私有”（private）区域和“共享”（shared）区域，所有服务器的“共享”区域共同构成了整个分离内存池。
![](https://pic1.imgdb.cn/item/686b61fd58cb8da5c8933eea.png)
**主要贡献**:
相比于物理内存池，逻辑内存池带来了显著的优势，这也是本文的主要贡献：
1. **更低的成本**: LMPs避免了物理池所需的额外硬件、机架空间和交换机端口，因为它们利用了服务器现有的基础设施。
2. **支持近内存计算 (Near-Memory Computing)**: 由于共享内存本身就位于某个服务器上，该服务器强大的CPU、GPU或加速器可以直接对这部分内存进行本地计算，无需额外硬件。这可以通过数据放置、数据迁移或计算迁移三种机制实现。
3. **更高的灵活性**: 每个服务器上私有和共享内存的比例可以根据工作负载的需求动态调整。这使得LMPs能够适应不同应用的需求，避免了物理池固定比例带来的僵化问题。
4. **更好的性能**: 当服务器访问的数据恰好位于其本地的“共享”区域时，访问速度是本地内存的速度，远快于访问远程内存。论文的评估表明，LMPs可以提供比物理池高出4.7倍的带宽。

基于这些优势，论文呼吁CXL社区应将研究重点从物理内存池转向逻辑内存池。

## 方法策略

**架构设计**:
* 在LMP架构中，服务器的内存被划分为两部分：
    * **私有内存 (Private Memory)**: 仅供本地处理器访问，用于存储操作系统、进程控制块、栈和堆等本地状态。
    * **共享区域 (Shared Regions)**: 可被全局任何服务器访问，所有服务器的共享区域共同构成了分离内存。
* 整个系统呈现出一个NUMA（非均匀内存访问）形态，其中内存访问分为延迟极低的本地访问和延迟较高的远程访问。
* 为了实现该架构，需要一个服务器端的运行时（runtime）和应用库，用于分配和管理分离内存。运行时还需要执行后台任务，以动态调整共享区域大小和执行数据迁移，从而优化数据局部性。

**遇到的困难与解决策略**:
论文明确指出了实现逻辑内存池面临的五大挑战及其初步解决思路：
1. **缓存一致性 (Cache Coherence)**: 在多服务器间维护共享内存的缓存一致性难以扩展。
    * **策略**: 不为大部分共享内存提供硬件一致性，而是只提供少量（几GB）的一致性内存用于同步和协调。这可以减轻窥探过滤器的压力。同时，可以利用交换机中的一致性引擎来减少本地访问的开销。
2. **调整共享区域大小 (Sizing the shared regions)**: 如何为每个服务器的共享区域分配合理的大小是一个关键问题。分配过大会影响本地工作负载，过小则无法满足应用需求。
    * **策略**: 将其构建为一个全局优化问题，周期性地求解，目标是在优先满足高价值应用的同时最大化本地访问次数。
3. **局部性平衡 (Locality balancing)**: 类似于NUMA系统，LMPs需要通过数据迁移来最大化本地访问。由于LMPs中本地和远程访问的性能差距更大，平衡显得尤为重要。
    * **策略**: 使用性能计数器来识别访问缓慢的远程数据，并利用访问位来识别热点数据以决定迁移内容。
4. **地址翻译 (Address translation)**: 需要一种高效的机制将逻辑地址映射到其物理位置（即在哪台服务器上），同时支持数据迁移而不使地址失效。
    * **策略**: 提出一种两步翻译方案：第一步进行粗粒度的映射，将逻辑地址映射到目标服务器；第二步在目标服务器内部进行细粒度的本地解析。
5. **故障域 (Failure domains)**: 服务器宕机会导致部分共享内存池的丢失。
    * **策略**: 可以借鉴为物理池提出的解决方案，如通过复制或纠删码来掩盖故障，或者通过异常向应用程序报告故障。

## 实验设置与实验结果

* **实验平台**:
    * 实验并非在真实的CXL硬件上进行，因为CXL 3.0 Fabric尚未上市。
    * 实验平台是一个双路服务器（Intel Xeon Gold 5120 CPU），使用其NUMA架构来模拟CXL Fabric。两个CPU Socket之间的UPI链路被用作模拟的远程连接。
    * 为了更真实地模拟CXL的预期性能，作者通过降低远程CPU的uncore频率来人为减慢一个UPI链路（称为Link1），并将其与未减慢的基线链路（Link0）进行比较。Link1被认为是更接近未来真实CXL性能的近似值。

* **实验设置与对比公平性**:
    * **对比对象**: 实验对比了三种内存池配置：`Logical`（论文提出的LMP方案）、`Physical cache`（带本地缓存的物理池）、`Physical no-cache`（不带本地缓存的物理池）。
    * **对比指标**: 主要的性能指标是聚合带宽（Aggregation BW (GB/s)）。延迟（Latency）也被讨论。
    * **公平性**: 为了公平对比，所有配置的总内存预算被严格控制为相同的值（96GB）。在物理池配置中，64GB分配给池，每个服务器保留8GB本地内存。在逻辑池配置中，96GB被均匀分配给4个服务器，每个服务器24GB。这种设置使得对比的不是内存总量，而是不同架构对内存的利用效率和性能，因此是公平且有意义的。

* **实验结果与效果**:
![](https://pic1.imgdb.cn/item/686b623358cb8da5c8933f06.png)
![](https://pic1.imgdb.cn/item/686b625158cb8da5c8933f0f.png)
    * **性能优势（解决问题效果好）**:
        * 当数据可以完全放入一个LMP服务器的本地内存时（8GB和24GB向量），LMP的带宽最高是`Physical no-cache`的4.7倍，是`Physical cache`的3.4倍。这是因为LMP可以直接进行高速的本地内存访问。
        * 即使数据量超过单个服务器的本地内存（64GB向量），LMP仍然有性能优势，因为它能本地访问一部分数据（3/8），在Link1上带宽高出`Physical cache` 42%。
    * **合理性分析（取得这样效果是合理的）**:
        * 结果的合理性在于，随着远程链路（模拟CXL）变得越慢，LMP中不受影响的本地访问性能所带来的优势就越明显。
    * **优势展示（比相关工作好、消融实验）**:
        * 对比`Physical cache`和`Physical no-cache`可以看作是对物理池方案的一种消融分析，表明即使物理池使用了缓存优化，LMP依然更优。
        * 在96GB向量的实验中，物理池由于池容量（64GB）不足而无法运行该工作负载。而LMP则可以通过动态调整，让每个服务器贡献全部内存，从而成功容纳并运行该任务。这极好地展示了LMP在灵活性上的绝对优势，解决了物理池无法解决的问题。

## 前提假设与局限性

**前提假设**:
1. **技术可行性**: 论文的一个核心假设是，未来的CXL或类似技术将支持共享的Fabric-Attached Memory (Shared FAMs)，并且允许一台服务器访问另一台服务器主板上的物理内存。这是实现LMP的技术基础。
2. **应用容忍度**: 假设应用和系统可以接受大部分共享内存不提供硬件缓存一致性的设定，仅依赖少量一致性内存进行协调。
3. **近内存计算的价值**: 假设通过数据放置、迁移和计算迁移实现的近内存计算能带来显著的性能收益，从而证明LMP架构的优越性。

**局限性**:
    
- 失去了原本物理池灵活拓展容量的能力。


## 物理池和逻辑池的对比
| 特性 (Feature) | 物理内存池 (Physical Memory Pool)                           | 逻辑内存池 (Logical Memory Pool)                            |
| :----------- | :----------------------------------------------------- | :----------------------------------------------------- |
| **基本理念**     | 一个与所有服务器物理上分离的、独立的共享内存硬件盒子。                            | 通过划分每台服务器的部分本地内存，共同组成一个逻辑上的共享池。                        |
| **主要优点**     | 允许内存和计算资源进行更独立的扩展。                                     | 成本更低、支持近内存计算、且在调整内存比例上更灵活。                             |
| **主要缺点**     | 部署成本高、有性能损失、缺乏近内存计算能力、内存比例固定僵化。                        | 需要解决缓存一致性、区域大小划分、局部性平衡、地址翻译和故障域等分布式系统挑战。               |
| **部署成本**     | 需要额外的硬件（电源、主板、CPU/ASIC）、机架空间和交换机端口。                    | 利用服务器现有的机架基础设施，不需要额外的机架空间或交换机端口。                       |
| **性能**       | 所有对池的访问都是远程访问，延迟比本地内存高3-5倍，带宽低4-10倍。                   | 当访问的数据位于本地服务器的共享区时，能以本地内存速度访问。实验中带宽最高可提升4.7倍。          |
| **灵活性**      | 本地与池化内存的比例在部署后是固定的，难以调整。                               | 服务器可以动态调整其私有和共享内存的比例，以适应工作负载需求。                        |
| **近内存计算**    | 不可行，或需要为内存池额外配置昂贵的处理硬件。                                | 可利用服务器本身强大的CPU/GPU直接对本地共享区的数据进行计算，无需额外硬件。              |
| **架构挑战**     | 主要挑战在于如何缓解远程访问带来的性能损失，以及可能出现的“Incast”拥塞问题。             | 需要设计复杂的运行时系统来管理缓存一致性、动态调整区域大小、进行数据迁移以平衡局部性、和高效的地址翻译。   |
| **共同点**      | 两者都通过CXL Fabric连接，都为服务器提供一个全局的地址空间，并使用CPU的加载/存储指令进行访问。 | 两者都通过CXL Fabric连接，都为服务器提供一个全局的地址空间，并使用CPU的加载/存储指令进行访问。 |
