
# A Case Against CXL Memory Pooling

## 作者与出版物
### 作者
- Philip Levis Google plevis@google.com
- Kun Lin  Google linkun@google.com
- Amy Tai  Google amytai@google.com

### 出版物
HotNets ’23

## 背景
在数据中心和云服务器中，计算资源和内存资源紧耦合，同时内存占据高昂的服务器成本（Azure为50%）。如果服务器内存不足，可能会出现空闲核心但没有可用内存的情况（核心闲置）；如果内存过多，则可能出现空闲内存而核心不使用的情况（内存闲置。鉴于此种原因，学术界提出了内存解耦/内存池化的方法，即服务器拥有自己的本地 RAM，足以满足平均或预期使用量。如果服务器需要更多内存或存在闲置核心，它可以从多个服务器共享的池中进行分配。

CXL协议有望为远程内存（内存池）提供低延迟、缓存一致性的访问，已经被视为一个内存池方案的最佳标准。

## 本文观察
虽然CXL有着种种优点，但是本文认为CXL内存池化面临三个严峻的问题，在层层问题的叠加之下，CXL内存池化成为了奢想。
三个问题如下：
- 成本：CXL内存池化所需的机架以及新的CXL内存成本可能超过给服务器增加DRAM的成本
- 软件复杂性：当前真实CXL硬件的延迟比理想延迟要高，进行内存池化还需要CXL交换机，这必然也会带来延迟的增加。因此，在高延迟的情况下，想要软件获得良好的性能，需要重写软件来显示管理CXL内存(类似于cuda cpoy)，并将块复制到本地DRAM中，这种显示、条件性和普遍性的内存管理增加了软件的复杂性。
- 实用性有限：现代服务器相对于大多数虚拟机来说已经非常大了 ；即使是简单的虚拟机打包算法也很少会留下搁浅内存，这从根本上削弱了内存池化的主要动机。

作者认为只要上述问题依然存在，利用CXL内存进行CXL内存池化就是一个奢想。

## 背景知识
### CXL延迟组成
![](https://pic1.imgdb.cn/item/685dfa6858cb8da5c874c73c.png)
执行一条load指令访问CXL设备，延迟如图所示

- 阶段一：在服务器内部的发起 (左上部分)
  1. Instruction Stream & movz...: 这一切始于 CPU 执行的一条指令，例如 movz，它需要从内存中加载数据。
  2. Cache and memory lookups: CPU 首先会检查自己的高速缓存（L1, L2, L3 cache）里有没有需要的数据
  3. Interconnect: 检查未命中后，由于这是一个双插槽（dual-socket）服务器，请求可能需要通过CPU之间的内部互连总线（Interconnect）发送到另一个CPU，因为CXL设备可能连接在另一个CPU上。这个跨CPU的通信本身就会增加延迟
  4. Cache and memory lookups: 在另一个CPU上，再次检查其本地缓存和直连的内存（DRAM）
  5. Request generation: 当在所有本地缓存和本地内存中都找不到数据时，系统确认该数据位于远端的CXL内存池中，于是生成一个正式的CXL请求
- 阶段二：请求的打包与远征 (左下部分 - CXL Overhead)
   1. TL processing (事务层处理): 将内存读写请求转换成CXL事务
   2. CXL flit packing (CXL流控单元打包): 将事务数据切割并打包成称为“flit”的标准大小数据块
   3. VCS, vPPB, CRC, Credits, Replay: 这是协议的链路层，负责虚拟信道管理、添加CRC校验码以确保数据准确性、流量控制（Credits）以及错误重传（Replay）等功能 
   4. Framing, Encoding (成帧与编码): 将数据打包成帧，并进行物理层编码，使其适合在电缆上传输 
   5. Transmission: 最后，通过物理接口将编码后的信号发送出去 
- 阶段三：跨越鸿沟 (中间部分)
  1. Cable propagation delay (线缆传播延迟): 信号在物理电缆中传输需要时间，这是由物理距离决定的、不可避免的延迟
- 阶段四：在内存池的接收与拆包 (右下部分 - CXL Overhead)
  1. Reception, Decoding, Deframing (接收、解码、解帧): 接收物理信号，将其解码并还原成数据帧
  2. VCS, vPPB...: 再次经过链路层处理
  3. CXL flit unpacking: 将flit重新组装成CXL事务
  4. TL processing: 将CXL事务还原成内存控制器能理解的读写请求
- 阶段五：在内存池内部的寻址与访问 (右上部分)
  1. CXL switch (CXL交换机): 因为内存池可能为多台服务器服务，所以内部需要一个交换机来路由请求。这个交换过程会引入额外的延迟
  2. Translate to MMU (转换为MMU地址): 内存池的控制器将请求的地址转换为其内部DRAM芯片上的物理地址
  3. DRAM access (DRAM访问): 最后，从DRAM颗粒中真正读取到所需的数据。为了最大化带宽，读取操作通常会跨多个DDR插槽进行

- 阶段六：数据的返回之路
  - 图的说明中提到：“响应会经过相同的路径” 。这意味着，读取到的数据需要重新经历一次完整的打包、物理传输、拆包的过程，才能最终返回到发起请求的CPU。因此，总延迟是这一整个来回旅程的耗时总和

### Pond回顾
- **问题**: 云服务商的服务器上存在大量“搁浅内存”（Stranded Memory），即因为 CPU 核心已经分配完，导致剩余的内存无法再分配给新的虚拟机，造成浪费
- **解决方案**: 不要给每台服务器配满内存，而是将一部分（例如 25%）内存抽出来，放到一个由多台服务器共享的 CXL 内存池里
- **效果**: 由于不是所有服务器都会同时达到内存使用高峰，通过“统计复用”（即共享），可以用更少的总内存来满足所有服务器的需求。Pond 的研究声称，这种方法可以节省 7-9% 的总内存，同时对大部分应用性能影响很小


## 对于文本的不赞同
- 成本：随着CXL工艺的成熟，成本必然会下降，CXL池化的收益将会远超DRAM的浪费。
- 软件复杂性：作者没有考虑到os kernel管理页面的情况，认为**显式搬运**才有好性能，事实是cuda都支持了隐式内存管理，更不用谈cxl这种统一内存的形式。
- 实用性有限：本文的数据来自于2019年，显然没有考虑到大模型时代，如果要在服务器上进行推理，如果每台服务器都配备近T级别的内存，但是峰值利用率又不高，这显然会造成内存搁置。