
# Performance Evaluation on CXL-enabled Hybrid  Memory Pool

## 1. 文章背景

随着深度学习训练、大数据处理以及内存数据库等现代化应用的发展，对内存的需求日益增长，常常超出单台机器的内存容量。研究表明，在谷歌和阿里巴巴等公有云供应商中，有高达一半的内存硬件资源未被充分利用。为了解决资源利用率低和成本高的问题，内存池化（Memory Disaggregation）方案被提出，它将内存资源从服务器中分离出来，进行统一管理和扩展。

新兴的 Compute Express Link (CXL) 互连技术提供了一种高速缓存一致性的连接方式，允许 CPU 以接近 DRAM 的延迟访问池化内存，成为实现内存池化的一个极具前景的方案。然而，即便实现了内存池化，DRAM 本身仍然是云服务器成本的主要部分，约占服务器成本的一半，且功耗远高于 SSD。因此，本文研究将成本更低、功耗更小的 NVMe SSD 引入到 CXL 内存池中，构建 DRAM-SSD 混合内存池，以期在不牺牲应用性能的前提下进一步降低云服务器的成本。

## 2. 相关工作以及局限性

**相关工作**

- **基于 RDMA 的内存池化**：在 CXL 出现之前，研究者们提出了基于远程直接内存访问（RDMA）的方案。RDMA 通过内核旁路和零拷贝技术提供高吞吐和低延迟的内存访问。
- **其他缓存一致性互连技术**：除 CXL 外，还有 NVLink、Gen-Z 和 OpenCAPI 等其他缓存一致性解决方案。
- **基于 CXL 的内存管理**：Li 等人利用机器学习预测器来分配虚拟机在本地和 CXL 内存池中的内存，以达到可容忍的性能下降。TMO 则通过监控性能下降，动态地将应用的冷数据页卸载到更便宜的存储介质上。
- **基于 FPGA 的原型**：近期有研究提出了基于 FPGA 的 CXL 内存池化原型，如 DirectCXL 和 ThymesisFlow，这些研究表明通过结合本地和远程内存池，一些应用的性能几乎没有衰减。

**局限性**

- **RDMA 的局限性**：RDMA 并非缓存一致性的，当缓存满时，其访问延迟会远高于本地 DRAM，并且仍有传统 DMA 式数据传输的开销，因此未被部署在对性能要求苛刻的云环境中。
- **现有研究的空白**：作者指出，现有的工作没有研究过多层级、多介质（如 DRAM 和 SSD）组成的混合内存池对应用程序性能的具体影响。

## 3. 论文发现以及论文贡献

**论文发现**

论文的核心发现是，采用 DRAM-SSD 混合内存池对应用性能的影响因工作负载的类型而异。

- 对于**计算密集型工作负载**（如视频处理 FFmpeg 和深度学习训练 ResNet50），性能瓶颈在于计算而非内存访问延迟，因此增加 SSD 作为内存扩展几乎不影响性能，同时能显著降低成本。例如，在内存过载率（Overcommit Ratio）为 2 时，训练 ResNet50 的性能仅下降了 2.68%。
- 对于**I/O 密集型工作负载**（如数据库 TPC-C 和数据分析 TPC-H），它们对内存延迟和带宽非常敏感，性能会因混合内存池的引入而受到严重影响。

**论文贡献**

本文的贡献主要有三点：

1. 设计了一个有效的软件模拟器：在缺少真实 CXL 硬件的情况下，利用缓存一致性的 NUMA 架构设计并实现了一个 CXL 混合内存池的软件模拟器。
2. 评估了代表性工作负载：在模拟器上评估了四种代表性的云应用（视频处理、数据库、数据分析、深度学习训练），展示了与纯 DRAM 内存池相比，采用混合内存池带来的性能变化。
3. 分析了性能与成本的权衡：通过改变混合内存池中 DRAM 和 SSD 的构成比例，揭示了不同应用在性能和成本之间的权衡关系。

## 4. 方法策略

- **目标**：验证并量化在 CXL 内存池中引入 SSD 以降低成本的可行性及其对不同应用的性能影响。
- **遇到的困难**：最主要的困难是当前缺乏可用的真实 CXL 硬件来搭建实验平台。
- **设计的方法与策略**：
  1. **利用 NUMA 架构进行模拟**：为了克服硬件缺失的困难，研究者选择利用现有多插槽服务器上广泛存在的非统一内存访问（NUMA）架构来模拟 CXL 内存池。选择 NUMA 的原因有二：首先，NUMA 架构是缓存一致性的，并且像 CXL 一样使用 load/store 指令进行内存访问。其次，从主 CPU 的角度看，访问远程 NUMA 节点的内存延迟与访问 CXL 附加内存的延迟相似。
  2. **构建分层内存系统**：
     - **本地内存（第一层）**：将 NUMA 节点 1 作为虚拟机的本地内存，提供最低的访问延迟。
     - **混合内存池（第二层）**：将 NUMA 节点 2 的 DRAM 与一块 NVMe SSD 共同组成一个混合内存池。这一层本身又是一个子分层系统：NUMA 节点 2 的 DRAM 是其中的高速缓存层，NVMe SSD 是低速存储层。
  3. **模拟页面迁移**：
     - **DRAM 与 SSD 之间**：利用 Linux swap 机制来模拟混合内存池内部（即 NUMA 节点 2 与 NVMe SSD 之间）的页面换入换出。当 NUMA 节点 2 的内存不足时，页面会被换出到 SSD 上。
     - **本地与远程之间**：页面可以在 NUMA 节点 1 和内存池之间迁移，由 Hypervisor 支持。
  4. **内存分配控制**：研究者修改了 Linux cgroups，为每个内存层级（本地 DRAM、远程 DRAM、SSD）设置独立的内存限制，从而精确控制应用程序在不同层级上可以使用的内存量。

## 5. 实验设置与实验结果

- **实验平台**：实验平台是基于商用硬件的软件模拟器，而非真实的 CXL ASIC 或 FPGA。平台配置为一台双插槽 Linux 服务器，搭载两颗 Intel Platinum 8268 CPU，384GB DDR4 DRAM，以及一块 3.5TB 的三星 PM983 SSD。通过 NUMA 架构模拟 CXL 内存池的行为。
- **实验设计**：
  - **过载率 (Overcommit Ratio)**：为了量化 SSD 引入的影响，论文定义了“过载率” (R)。实际使用的 DRAM 量 = 应用所需内存量 / R。R = 1 表示纯 DRAM 内存池，R > 1 则表示部分内存需求由 SSD 满足。实验中 R 的取值分别为 1, 1.5, 2, 3, 4。
  - **对比指标**：
     - **性能**：针对不同负载使用不同指标，包括 FFmpeg 的帧率 (FPS)、TPC-C 的每分钟事务数 (tpmC)、TPC-H 的查询累积运行时间和 ResNet50 的训练时间。
     - **开销**：通过测量页面错误（Page Fault）数量来解释性能变化的原因。
     - **成本**：计算总内存成本（Total Cost of Memory, TCM）的节省百分比。
  - **对比公平性与全面性**：
     - **公平性**：所有混合内存池的实验结果都与过载率为 1（即纯 DRAM 池）的基线进行比较，这是一个公平的基准。
     - **全面性**：选取了视频处理、在线事务处理、数据分析和深度学习四种有代表性的云工作负载，覆盖了不同类型的应用场景。
  - **消融实验**：通过不断增加过载率（从 1.5 到 4），实验系统性地展示了增加 SSD 比例对性能和成本的影响，这可以看作是一种消融分析，证明了性能的下降和成本的节省是由 SSD 的引入直接导致的。
- **实验结果**：
  - **FFmpeg (视频编码)**：性能下降幅度很小。即使过载率为 4，性能也仅下降 11.9%，因为它是计算密集型应用。
  - **ResNet50 (深度学习)**：性能同样受影响较小。过载率为 4 时，性能下降 17.9%。
  - **TPC-C (数据库)**：性能下降非常剧烈。过载率仅为 1.5 时，性能就骤降 55.8%，在过载率为 4 时下降了 78.5%。
  - **TPC-H (数据分析)**：性能同样严重下降。过载率为 1.5 时性能下降 1.13 倍，过载率为 4 时下降 5.32 倍。
  - **成本节省**：引入 SSD 可以显著节约成本。过载率为 1.5 时即可节省 32% 的总内存成本，当过载率为 4 时，可节省高达 72.5%。

## 6. 前提假设与局限性

- **前提假设**：
  1. **NUMA 模拟的有效性**：文章假设远程 NUMA 节点的访问特性（如延迟和缓存一致性）可以作为 CXL 连接内存的合理近似。
  2. **Swap 模拟的合理性**：假设使用 Linux swap 机制来模拟 DRAM 和 SSD 之间的数据迁移是可行的，因为在 SSD 延迟面前，页面错误处理本身的软件开销可以被忽略。
- **局限性**：
  1. **模拟器而非真实硬件**：这是一个软件模拟，而非真实硬件。作者承认，使用真实的 CXL 硬件可能会获得更好的性能，因为模拟中主机端仍然存在页面错误处理的开销。
  2. **简单的页面管理策略**：实验使用了 Linux 默认的预取器，该预取器只能检测顺序访问模式。作者推测，TPC-C 和 TPC-H 的糟糕表现并不意味着它们完全不能用于混合内存池，如果采用更智能的页面迁移和缓存算法，性能可能会得到改善。
