
# Pond

## Abstract
云服务商们即希望硬件能够满足性能要求，同时也要降低成本，其中内存就是影响性能和成本的关键因素。恰好内存池化可以提高DRAM利用率，从而降低成本。但是在云服务器上实现池化是个有挑战性的事。
>内存池化是指将大量的内存进行池化，形成一个内存池，CPU在内存池中动态申请内存进行使用。
本文提出了Pond，这是首个既能满足云性能目标又能显著降低DRAM成本的内存池系统。Pond基于CXL实现了对内存池中内存的Load/Store语义访问，同时又提出了两个观察：
- 跨8-16个插槽的池化足以实现大部分收益。这使得小规模池设计能够实现低访问延迟。
- 通过机器学习模型可以准确预测为虚拟机（VM）分配多少本地内存和池内存，以实现与同NUMA节点内存相近的性能。

Pond能够将DRAM成本降低7%，同时性能与同NUMA节点VM分配相比仅相差1-5%。

## Introduction
云服务商为了保证云客户虚拟机(VM)的性能，常常将VM的所有内存都预分配到与其核心相同的NUMA节点上。预分配和静态固定内存还便于使用虚拟化加速器。
>内存性能的黄金标准是访问由发出请求的核心所在的同一NUMA节点提供服务，从而实现数十纳秒的延迟。

由于DRAM的可拓展性较差且替代方案不成熟，DRAM已经成为了硬件的主要即成本。比如：
- Azure中，DRAM可占服务器成本的50%
- Meta占机架成本的40%

由于VM的内存分配策略，导致了核心在分配给VM时，该核心仍有剩余内存不会被租用，这会造成内存搁置。随着更多核心分配给VM，高达25%的DRAM会被搁置。

现有技术应对搁浅问题仍然具有挑战性，如：
- 现有池化技术（RMDA/PCIe），闲置内存可返回到内存池中供其它VM使用，但是由于技术原因，访问延迟过高，并且当VM访问池内存时，若数据未提前加载到本地，需触发页面错误，由虚拟机管理程序（Hypervisor）介入，通过软件从池中动态迁移数据，延迟进一步增加。

Pond，这是首个在公有云平台上同时实现同NUMA节点内存性能和成本竞争力的系统。Pond结合了硬件（CXL）和系统技术（支持CXL池化）。CXL的延迟还是高于NUMA节点，Pond引入了支持CXL池化的系统技术，显著降低了这种高延迟的影响。

Pond的可行性基于四项**关键洞察**：
- 8-16个插槽的池大小足以实现足够的DRAM节省。在8-16个插槽的池中，CXL会增加70-90ns的访问延迟，而在机架级池化中会增加超过180ns的延迟。
- 43%的工作负载在CXL池内存延迟增加64ns时性能损失不超过5%，但21%的工作负载性能下降超25%。这促使Pond采用小规模CXL池（8-16节点）结合机器学习预测，将延迟敏感型和非敏感型工作负载差异化分配，最终实现7%的DRAM节省且95%以上VM性能损失控制在5%以内。
- 约50%的VM访问了不到其租用内存的50%，这意味着存在大量"闲置内存"可被重新利用。Pond将池内存作为零核心虚拟NUMA（zNUMA），zNUMA 将池内存呈现为一个"有内存但无 CPU 核心"的特殊 NUMA 节点，操作系统自然地将活跃内存集中在有CPU的NUMA节点，zNUMA内存保持空闲，避免了传统统一地址空间中潜在的性能干扰。

    >传统内存池化方案（如 Kang 等人的研究）采用统一地址空间设计，会导致操作系统无差别地使用本地和池内存，造成性能敏感型应用不可避免地访问高延迟的池内存。
- Pond可以通过正确预测以下两点，以同NUMA节点性能分配CXL内存：a）VM是否对延迟敏感；b）VM的未访问内存量。
    - 对于预测错误的情况，Pond引入了一种新颖的监控系统，检测内存性能不佳并触发缓解措施，将VM迁移到仅使用同NUMA节点内存的配置。


**贡献**：
1. 首次公开大型公有云提供商的内存搁置和未访问内存特性分析。
2. 首次分析不同CXL内存池大小的有效性和延迟。
3. 首个基于CXL的完整内存池系统，适用于云部署且性能优异。
4. 数据中心规模的准确延迟和资源管理预测模型。这些模型可实现1-5%的可配置性能降级。
5. 广泛的评估验证了Pond的设计，包括zNUMA和预测模型在生产环境中的性能。分析表明，Pond通过跨16个插槽的池可以将DRAM需求减少7%，对应大型云提供商数亿美元的成本节约。


## Background
### CXL访存
最后一级缓存（LLC）对CXL内存地址的未命中会转换为CXL端口上的请求，其响应会引入缺失的缓存行（图1）。类似地，LLC写回会转换为CXL数据写入。这些操作均不涉及页面错误或DMA。
![](https://pic1.imgdb.cn/item/67eceebb0ba3d5a1d7ea10c0.png)

## Memmory stranding & workload sensitivity to memory latency
### Azure中的内存搁置
![](https://pic1.imgdb.cn/item/67ecf4c90ba3d5a1d7ea1592.png)
图2展示了在虚拟机（VM）环境中，随着计划分配的CPU核心百分比增加，搁置内存（未被有效利用的DRAM）的比例呈现上升趋势；中位数数据显示，在75% CPU利用率时搁置内存约为6%，而在85% CPU利用率时超过10%；此外，数据的变异性主要由不同VM混合引起，例如计算密集型VM即使在低CPU利用率下也可能导致高搁置内存，而误差条和异常值进一步表明，在高CPU利用率区间，个别情况下搁置内存比例甚至可达到30%，突显了资源分配不均衡与潜在优化需求。



Azure目前未实现内存池化。然而，通过分析其VM到服务器的跟踪数据，我们可以估计通过池化可以节省的DRAM量。图3展示了当VM以固定百分比（10%、30%或50%）的池DRAM调度时，池化DRAM的平均减少量。
![](https://pic1.imgdb.cn/item/67ecf9700ba3d5a1d7eaf524.png)

### Azure中VM的内存使用情况
总体而言，我们发现虽然VM内存使用情况因集群而异，但所有集群中都有相当一部分VM存在未访问内存。第50百分位数为50%的未访问内存。

从这一分析中，我们得出以下关键观察和对Pond的启示：
- VM内存使用情况差异很大。
- 在未访问内存最少的集群中，仍有超过50%的VM有超过20%的未访问内存。因此，有大量未访问内存可以无性能损失地解聚到池中。
- 挑战在于（1）预测VM可能有多少未访问内存；（2）将VM的访问限制在本地内存中。Pond解决了这两个问题。

### 工作负载对内存延迟的敏感性
![](https://pic1.imgdb.cn/item/67ecfb1d0ba3d5a1d7eaf5fb.png)