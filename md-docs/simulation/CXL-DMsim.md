
# CXL-DMsim


## 背景
内存解耦（Disaggregation）技术通过将内存从CPU中分离，实现内存资源池化和按需扩展，成为解决内存瓶颈和资源利用率低的有效方案。传统基于RDMA的远程内存访问延迟较高，难以满足需求。新兴的低延迟、高带宽缓存一致性互连协议（如CXL）被广泛看好，尤其是CXL协议支持多种内存技术（DRAM、NVM、Flash等），有助于构建统一的异构内存池。
但CXL仍处于初期阶段，缺乏成熟商品产品，研究受限于缺少高保真、系统级的仿真工具。现有软件仿真、软件模拟、硬件仿真和原型制作方法各有不足，如协议支持不全、仿真精度低、成本高昂等，亟需一种准确、灵活且成本可控的CXL内存系统仿真平台。
## 相关工作以及其局限性
当前开展CXL研究的工作主要有四种:
1. Software-Based Emulation
   - Qemu:主要用于功能级别的仿真和验证,不对CXL物理特性和内部微架构进行建模,不适用于基于CXL系统的架构探索和精确的性能评估
   - Mess: 
     - Mess [18] 的最新研究提出了一种新型内存基准，用于测量带宽-延迟曲线(1000次内存操作)。基于这些曲线，研究者们提出了一种通用的内存模拟器，旨在模拟不同类型主存储器的实时延迟，涵盖 DDR4、DDR5、HBM 和 CXL 内存。
     - Mess也有着和Qemu同样的局限,同时根据最近 1000 次内存操作（约 100 微秒）计算出的 cpuBW 值可能无法准确捕捉实际应用程序中的真实内存带宽 (messBW)
2. Software-Based Simulation
   - gem5-CXL:gem5-CXL 将模拟的 CXL 设备连接到 MemBus，而不是将其挂载到 IOBus 上来模拟外围设备。它没有对 CXL 协议（基于 PCIe PHY）的行为进行建模，并且缺乏与建模设备之间清晰的访问接口。该项目已经四年没有更新，目前处于停滞状态。
   - CXLMemSim:本质上不是一个完整的系统模拟器，因为它既没有对 CXL 协议和系统架构进行建模，也没有对相关的软件栈进行建模。它缺乏周期精确性，功能有限，导致可用性很差。
3. Hardware-Based Emulation
   1. 使用numa节点模拟:远程 NUMA 内存在模仿 CXL 内存方面缺乏真实性和准确性。《Demystifying CXL》中已强调，这两种内存的访问延迟和带宽存在显著差异，且它们的内存访问路径也不同。
4. Hardware Prototyping
   - DirectCXL
   - TPP
   - Demystifying CXL
   - NeoMem
   - 目前市场上还没有基于 CXL 的内存分解系统的成熟硬件原型。由于硅片开发周期长，现有硬件原型和商业设备生产成本高昂。此外，硬件原型在系统配置和观测方面也缺乏灵活性。

## 动机与贡献
现有仿真器、模拟器以及 CXL 硬件的局限性，需要一种全面且贴近实际的仿真解决方案，以便对 CXL 互联系统进行敏捷设计与评估，于是本文提出了CXL-DMsim，它是一款可配置、可扩展且经济高效的模拟器。
CXL-DMsim的主要贡献有：
- 一种灵活的 CXL 内存扩展器（Type 3 设备）的设备模型，目前支持 DRAM、NVM 和 Flash 作为底层存储介质。
- 支持 CXL.io 和 CXL.mem 子协议，用于在 CXL-DMSim 上枚举、配置和访问我们的 CXL 内存设备。
-  一个驱动程序，用于设备在应用程序管理模式下运行；以及一个 NUMA 感知的内存管理机制，用于在内核管理模式下运行。
- 广泛的芯片测量和评估，验证了 CXL-DMSim，包括性能测试、可用性和保真度测试、真实世界应用测试以及观察和可扩展性测试；这验证了系统的可行性，并为 CXL 解耦内存的合理使用提供了指导。

## 方法策略

### 1. CXL内存扩展设备模型设计  
![](https://pic1.imgdb.cn/item/68583d1858cb8da5c862ffe0.png)
- **设备模型结构**：CXL内存扩展器（Type 3设备）被设计为四层模块结构：  
  1) 响应端口（Response Port），用于与主机IO总线通信，接收请求包并发送响应包；  
  2) CXL控制器，负责解析CXL协议中的读写请求包（包括CXL.io和CXL.mem子协议），并进行地址转换；  
  3) 接口层（Interface Layer），实现请求转发，解耦前端CXL控制器和后端内存模块，方便替换不同存储介质；  
  4) 内存模块，包括内存控制器和存储介质（DRAM、NVM、Flash等），支持多种存储技术。  
- **事件驱动时序模拟**：为实现周期级准确性，采用事件驱动的时序模式，模拟请求包和响应包在FIFO队列中的排队和处理延迟，捕捉真实的访问时序和拥塞现象。  
- **参数可配置性**：通过配置设备协议延迟（dev_proto_lat）、请求和响应FIFO深度（req_fifo_depth、rsp_fifo_depth）等参数，灵活调整模拟器以匹配不同厂商的CXL设备性能特性。  
- **协议解析与地址映射**：CXL控制器解析协议包，基于PCI配置空间读取和写入设备寄存器（BAR），实现设备枚举和内存映射；对内存访问请求进行地址转换，转发至后端内存控制器。

### 2. CXL协议支持（CXL.io和CXL.mem）  
![](https://pic1.imgdb.cn/item/68583d5258cb8da5c862ffe8.png)
- **CXL.io协议**：复用gem5中已有的PCI协议实现，用于系统启动时枚举和配置CXL设备。具体包括读取设备内存大小、映射设备内存到主机物理地址空间，以及写入配置空间完成初始化。  
- **CXL.mem协议**：新增对CXL.mem子协议的支持，模拟主机CPU通过load/store指令访问CXL内存的过程。  
  - 在gem5中扩展包结构，将内存访问包转换成CXL.mem协议包（M2SReq、M2SRwD等），并通过桥接模块（Bridge）在内存总线和IO总线之间传递。  
  - 桥接模块实现请求和响应的FIFO缓冲，模拟PCIe物理链路的全双工传输和信用流控机制，防止死锁和拥塞。  
  - CXL控制器接收请求包，进行地址转换后转发给后端内存控制器，完成实际读写操作，响应包返回主机。
![](https://pic1.imgdb.cn/item/68583dcf58cb8da5c862fff5.png)  
- **协议时延建模**：协议处理延迟（host_proto_lat、dev_proto_lat）和FIFO队列深度参数共同决定请求在协议层的排队和处理时间，精确模拟协议带来的性能开销。

### 3. 操作系统层的内存管理策略  
![](https://pic1.imgdb.cn/item/68583d7e58cb8da5c862ffe9.png)
- **应用管理模式（AM）**：  
  - 设备驱动提供系统调用接口（如mmap），应用程序显式调用接口分配和释放CXL内存。  
  - 驱动内部维护一个双向链表管理内存块状态（空闲/占用）、所属进程PID、大小和偏移地址。  
  - 应用需修改代码以调用驱动接口，获得更灵活、细粒度的内存管理能力。  
- **内核管理模式（KM）**：  
  - CXL内存被操作系统内核识别为无CPU的NUMA节点，系统启动时通过修改e820表和内核NUMA初始化实现。  
  - 内核透明管理CXL内存分配，应用程序通过标准内存分配接口（malloc、numactl等）使用CXL内存，无需修改应用代码。  
  - 该模式符合现实硬件平台主流用法，但应用无法感知CXL内存的特性差异，仅通过NUMA距离体现性能差异。  
- **内存管理的灵活性与兼容性**：两种模式满足不同用户需求，AM模式适合研究和开发新特性，KM模式适合无侵入式应用部署。

### 4. 硬件测试平台校准与验证  
- 设计了包含FPGA和ASIC两种真实CXL内存设备的硬件测试平台，测量真实设备的延迟和带宽。  
- 利用硬件测试数据对模拟器关键参数进行校准和微调，确保模拟结果与真实设备高度一致，平均误差约3.4%。  
- 通过多种基准测试（LMbench、STREAM）和实际应用测试（Redis、Viper数据库）验证模拟器的准确性和实用性。  
- 通过模拟器深入分析高并发访问下的拥塞瓶颈，定位CXL控制器响应队列成为性能瓶颈，验证模拟器的高观测能力。


## 实验设置与实验结果


### 实验设置

该研究的实验在真实的硬件测试平台和其提出的CXL-DMSim模拟器两个环境中进行，以进行性能评估和模拟器保真度验证。

**1. 硬件测试平台**

* **主机**: 一台高性能的双插槽x86服务器。
* **CPU**: 两颗英特尔®至强®铂金 8468V处理器，每颗拥有48个物理核心。
* **本地内存 (DDR-L)**: 作为性能基准。为了与CXL链路进行公平比较，每个CPU插槽（NUMA节点）仅启用一个内存通道，并安装了一个32GB的DDR5 4800MT/s内存条。
* **被测CXL内存设备**:
    * **CXL-FPGA**: 这是一个基于英特尔® Agilex™ I系列FPGA开发套件的CXL设备，集成了硬件CXL IP。它通过CXL 1.1接口连接，后端搭载16GB的DDR4 3200MT/s内存。
    * **CXL-ASIC**: 这是一款该团队内部开发的基于ASIC的CXL内存设备。它拥有两个CXL内存控制器，后端为DDR5 4800MT/s内存，实验中使用了64GB容量。
* **模拟CXL (DDR-R)**: 实验还测量了访问远程NUMA节点上的DDR5内存的性能，以此作为以往研究中模拟CXL方法的对比项。

**2. CXL-DMSim 模拟器**

* **基础**: 基于gem5 v23.1版本，在全系统（Full-System）模式下实现。
* **模拟主机**:
    * **CPU**: 模拟了一个拥有48个核心的X8603CPU。
    * **本地内存 (CXL-DMSimL)**: 32GB的单通道DDR5 4400MT/s内存。
* **模拟CXL设备**:
    * **CXL-DMSimF**: 模拟CXL-FPGA设备，配置了16GB的DDR4 3200后端内存。
    * **CXL-DMSimA**: 模拟CXL-ASIC设备，配置了64GB的DDR5 4400后端内存。
* **软件栈**: 在模拟器中运行的客户机操作系统（Guest OS）是修改版的Linux v5.4.49，包含了为其CXL设备开发的专用驱动程序。



### 实验结果

实验结果按照论文中的评估顺序，从模拟器保真度、CXL内存优势、以及模拟器的高级能力等几个方面展开。

**1. CXL-DMSim的可用性与保真度评估**

* **内存延迟测试 (LMbench)**:
  ![](https://pic1.imgdb.cn/item/68583e4058cb8da5c863000c.png)
    * 实验使用LMbench测量了随机读取延迟。结果显示，CXL-FPGA的延迟约为本地DDR-L的2.88倍，CXL-ASIC的延迟约为DDR-L的2.18倍。实验证明，CXL-DMSim能准确模拟真实CXL设备的延迟特性，而传统的远程NUMA模拟方法则不够准确。
* **内存带宽测试 (STREAM)**:
  ![](https://pic1.imgdb.cn/item/6858f1ee58cb8da5c864ceca.png)
    * 实验使用STREAM测量了可持续内存带宽。结果显示，CXL-FPGA的带宽达到了本地DDR-L的45%-69%，而CXL-ASIC的带宽达到了DDR-L的82%-83%。CXL-DMSim的模拟结果与硬件实测值非常接近，平均建模错误率约为6%。
* **真实世界应用测试 (Redis)**:
  ![](https://pic1.imgdb.cn/item/6858f20958cb8da5c864d0a5.png)
    * 实验使用Redis评估了延迟敏感型应用的性能。结果表明，仅使用本地DDR-L时性能最高。由于Redis对微秒级的延迟非常敏感，即使混合少量高延迟的CXL内存也会导致显著的性能下降。

**2. CXL内存优势探索**

* **内存容量扩展 (Viper)**:
   ![](https://pic1.imgdb.cn/item/68583e7658cb8da5c863000e.png)
    * 实验使用Viper键值数据库来评估CXL的扩容效果。当本地内存不足时，使用CXL扩展内存可以将系统吞吐量（QPS）提升最多23倍。这证明了CXL是解决内存容量瓶颈的有效手段。
* **内存带宽扩展 (DLRM/MERCI)**:
    ![](https://pic1.imgdb.cn/item/6858f26558cb8da5c864d5fb.png)
    * 实验使用DLRM推荐模型来评估CXL对带宽敏感型应用的好处。结果显示，将DDR-L与CXL-ASIC内存以50%:50%的比例进行交织分配，系统性能提升了约60%。这是因为交织策略有效利用了多个设备的并行带宽，减少了访问竞争。

**3. CXL-DMSim的高级能力展示**

* **拥塞分析 (Congestion Analysis)**:
    ![](https://pic1.imgdb.cn/item/6858f2a458cb8da5c864d80d.png)
    * 模拟器被用来深入分析在高并发（48核 vs 12核）下MERCI工作负载性能下降的原因。通过模拟器的高可观测性，实验精确定位了性能瓶颈的根源在于CXL控制器的**响应队列**发生拥塞，并产生背压，最终耗尽了L3缓存的MSHR条目。
* **可扩展性 (Expandability)**:
    ![](https://pic1.imgdb.cn/item/6858f2be58cb8da5c864d8d5.png)
    ![](https://pic1.imgdb.cn/item/6858f2d058cb8da5c864d964.png)
    * 为了展示模拟器的可扩展性，研究者构建了一个CXL-SSD设备模型。模型在CXL控制器和SSD后端之间设计了一个缓存层，以弥补DRAM和SSD之间的性能鸿沟。结果表明，加入缓存后，CXL-SSD的性能得到了显著增强，达到了CXL-DRAM性能的72%-88%。这证明了CXL-DMSim可以方便地扩展以支持不同类型的存储介质。


## 假设与局限(缺点)

### 实验前提假设
- 模拟基于CXL 1.0/1.1协议，主要针对Type 3内存扩展设备。
- CPU模型为gem5中的X86O3CPU，性能和架构与真实CPU存在差异，影响绝对精度。
- 当前仅支持单主机模拟，不支持多主机和复杂拓扑。
- Loacl DDR为单通道，且不能满足应用的容量
- 内核管理模式将CXL内存作为NUMA节点，应用层无法直接感知内存特性差异。

### 局限性
文章在考虑local ddr和loadl ddr 与cxl memory混合的比较时，local ddr是单条内存，而local + cxl memory是交错（interleaved）内存的形式，如果local ddr采用双通道（interleaved的形式），必然会比原本的单通道local ddr的性能要高，应该考虑的是在拓展cxl-memory对比单纯增加local ddr的情况下，有多少的性能下降，会节省多少TCO(鉴于市面上还没有对于的CXL内存设备，TCO可以省略)。